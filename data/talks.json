{
  "growing-the-next-generation-of-us-how-to-make-sure-this-industry-stays-in-good-hands": {
    "title": "Growing the Next Generation of us: How to make sure this industry stays in good hands",
    "description": "As the world clamors for more video to stream, many of us are asked to rapidly scale our teams with people who are just as passionate as we are. <br><br>In this presentation, we’ll cover the topic of how to onboard someone to the video streaming space. We know this industry is dense and there is no escaping its learning curve but positioning the next generation to succeed starts the day they do.<br><br>We’ll highlight how to make the technology approachable and excite someone who is new to the space without overwhelming them. We will talk strategy to identify and partition projects for engineers so they can learn fast, develop the trust of their peers and become a key contributor with a little help from their teammates. In this talk, the audience will take away how they too can grow fresh talent by leading newcomers down a path where they are constantly learning and discovering, alongside the rest of us, just how deep this rabbit hole goes. ",
    "speakers": [
      "Alexandria Shealy",
      "Kevin Fuhrman"
    ]
  },
  "dont-let-latency-ruin-your-longtail-an-introduction-to-dref-mp4-caching": {
    "title": "Don't let latency ruin your longtail: an introduction to \"dref MP4\" caching",
    "description": "Many setups in large scale on-demand video streaming nowadays rely on a just-in-time packager to deliver remotely stored MP4 content. Unfortunately, the latency to such a storage backend can be suboptimal and a just-in-time packager needs to make a relatively high number of requests to the source content to package it dynamically. This will impact the overall performance of the packager, as well as the start-up delay that customers will experience when a stream that hasn&#39;t been cached on the CDN is requested (i.e., longtail content).<br><br>In my talk, I will propose how the number of requests to the remote storage backend can be minimized by using a novel but spec compliant approach to packaging MP4. In essence, it&#39;s about generating an additional MP4 that acts as an intermediary between the packager and the source content. This MP4 does not contain media data, but merely references the source content using MP4&#39;s &#39;dref&#39; box. What this &#39;dref MP4&#39; does contain, is the index information (sample tables) of the original track, stored in a &#39;moov&#39; box. This is the small but vital bit of information that a just-in-time packager needs to handle each and every incoming request. <br><br>By omitting the original media data, a dref MP4 becomes very small and can be cached easily in a reverse proxy cache that sits between the just-in-time packager and the remote storage backend. Or it can be stored locally on the packager itself. Either way, this approach will ensure that the just-in-time packager will have much faster access to information that it needs to access very often.<br><br>The impact of this approach is considerable. In my tests with Unified Origin, Unified Streaming&#39;s just-in-time packager, an MPD (DASH client manifest) was generated 70x times faster (as it contains timelines for all track in a stream, a packager a will need to read the index information of each of those tracks). The gains when packaging a media segment were much smaller, but still quite significant with an improvement of 1.5x. Also, the general throughput of the packager improved by ~10-20%.<br><br>In my talk I will explain the above by going into the nitty gritty details of the relatively high number of requests that Unified Origin make to a storage backend, explaining what each of these requests are for to give the audience a more thorough understanding of the inner workings of a just-in-time packager in general, and Unified Origin in specific, while also introducing them to the concept of a dref MP4, which – based on my experience thus far – will be a format that most have never heard of.<br><br>With this unique blend of technical insight, practical gains and an introduction to a novel type of MP4, I think this talk will serve the audience at Demuxed particularly well. I hope you do, too.",
    "speakers": [
      "Boy van Dijk"
    ]
  },
  "the-hypothetical-reference-decoder-or-vbvbitrate-is-not-bitrate": {
    "title": "The Hypothetical Reference Decoder. Or: vbv-bitrate is not bitrate",
    "description": "Some of you might have already stumled on HRD parameters in h.264/AVC or h.265/HEVC. In the open source encoders x264 and x265 these can be enabled using the --vbv-bitrate and --vbv-bufsize options. Also the AV1 standard specifies a buffer model. While these buffer models were designed with fixed transmission lines and hardware limitations in mind, we see some use of them in modern applications for different reasons (like influencing the rate control). In this talk I would like to explain in depth what the HRD is and what the model describes. I want to explain why you must not think of bitrate when setting vbv-bitrate, what the connection to profiles/levels is and how these parameters influence and limit your encoder. ",
    "speakers": [
      "Christian Feldmann"
    ]
  },
  "building-an-8k-encoder--live-streaming-platform-using-off-the-shelf-stuff-in--10-minutes": {
    "title": "Building an 8k encoder + live streaming platform using off the shelf stuff in < 10 minutes",
    "description": "Coming Soon",
    "speakers": [
      "Colleen Henry",
      "Caitlin O'Callaghan"
    ]
  },
  "providing-better-video-experiences-for-the-next-billion-users-what-facebook-learned-from-focusing-on-improving-video-playback-in-india": {
    "title": "Providing better video experiences for the next billion users: What Facebook learned from focusing on improving video playback in India",
    "description": "When our team set out to improve the video playback experience for Facebook users accessing our service in India, we looked broadly at the issues that mattered most to users and set off to fix those issues. In this talk, I’ll discuss our approach to identifying problems, defining success metrics, and improving our infrastructure across the networking stack, our video encodings, and video players. While we focused on solutions that would have the highest impact for users in India, many of these changes improved playback performance for our users worldwide.<br><br>The first step was to understand the state of the video playback experience in India. To do this, we leveraged a tool that allows us to root cause stalls from client to the CDN and quantify the opportunity size of various changes. We paired this data analysis with on the ground user research. We sent researchers and engineers to India to meet with users to observe how they consumed video on their devices. Based on our analysis and research, we identified the top areas to invest in and targeted metrics to track success.<br><br>On the network side, we saw meaningful improvements from enabling QUIC in our mobile apps. On the encoding side, we drove improvements by improving coverage of low bitrate lanes and by improving compression efficiency. On the playback side, we made improvements to our ABR logic and improved cache eviction logic and reduced internal network congestion.<br><br>Our work here is not complete, but we’ve made meaningful progress towards making Facebook videos perform well for users on all devices and networks.<br>",
    "speakers": [
      "Denise Noyes"
    ]
  },
  "how-big-buck-bunny-helped-track-down-bugs": {
    "title": "How Big Buck Bunny helped track down bugs",
    "description": "Media events are asynchronous and unpredictable, so why is it that we test our media features with mocks that behave both synchronously and predictably? In this presentation, we’ll explore a media testing strategy that’s built around playing real media in tests and discuss how these tests can uncover race conditions and integration failures in your code",
    "speakers": [
      "Evan Farina"
    ]
  },
  "greed-lies-and-deceit--curious-stories-of-player-misbehavior": {
    "title": "Greed, lies and deceit – curious stories of player misbehavior",
    "description": "A team of sleuths at Akamai have been tasked with uncovering anomalous behaviors in how video players interact with the CDN.<br><br>At their fingertips, these detectives have a fantastic dataset from which to look for clues and gather evidence…..MILLIONS (and even BILLIONS) of CDN log lines that include the world’s largest, latest and greatest OTT video services. No stone can be left unturned. Popular player platforms and applications are all under suspicion!<br><br>In this talk, we will share case details of these investigations including: <br>* examples of suspicious behavior involving  stream fetching, connection usage, and more<br>* the investigation methodology and the tools used for the job<br>* datasets and graphs to illustrate the above<br><br>And then you can decide....Are the suspect players greedy and deceitful, or are they cooperative and compliant participants of the video ecosystem?<br><br>We believe these investigations will lead to improved player/CDN interactions, resulting in better performance and efficiency for the video ecosystem that will ultimately benefit the end-user experience.",
    "speakers": [
      "Frank Paolino"
    ]
  },
  "cdn-logs-not-just-for-breakfast-anymore": {
    "title": "CDN Logs: Not just for breakfast anymore",
    "description": "Historically, CDN logs have been a necessary evil: something you have to gather in case something goes wrong and you need to trawl through and figure out why an end-user had a less than stellar experience. But as CDNs are starting to make this logging data available in near real-time, they become an almost invaluable tool for creating a feedback loop for actionable insights into your video delivery platform&#39;s performance.<br><br>This talk will demonstrate how companies are using CDN logs to create real-time performance monitoring dashboards that help identify issues before they become catastrophic for viewers. In addition, we&#39;ll take it a step further in showing the additional value when you join up CDN logging data with RUM data services such as Mux, NPAW or BitMovin, allowing you to look at an end-user&#39;s experience on a per request basis.",
    "speakers": [
      "Jim Hall"
    ]
  },
  "four-approaches-to-building-trick-view-scrubbing": {
    "title": "Four approaches to building trick view scrubbing",
    "description": "When presented with the requirement to build a &quot;thumbnail scrub&quot; feature we immediately were presented with a number of challenges to support across a dozen devices including gaming consoles and CTV.<br><br>This talk is how we iterated through four options of a keyframe API, Sprite Sheet API webVTT and video I-FRAMES playlist as each presented benefits and disadvantages I&#39;ll cover why we chose in the end to support them all (spoiler gaming consoles aren&#39;t great).",
    "speakers": [
      "Jeremy Brown"
    ]
  },
  "hey-viddy-hey-viddy-yay-speak-description-to-me": {
    "title": "Hey Viddy, hey viddy, yay! Speak Description to Me",
    "description": "Some background on what&#39;s special about my talk and why you know you&#39;ll want it. I&#39;m a 20/10 fully sighted person. My wife&#39;s vision has progressively gotten worse since I met her and she is now legally blind. We like to watch streaming vids together but it continued to get progressively difficult and now we really can&#39;t watch anything together unless it has audio description or I become the audio description that night.<br><br>The number of movies and shows out there with audio description is abysmal. The Audio Description Project states that as of Aug 14, 2020; Apple+ has 35 titles. In 2020, Apple has added a net of about 3 titles each month. Seriously? And this is an industry thing, not an Apple thing.<br><br>So, let&#39;s get together and talk about what we&#39;re all passionate about, video streaming, and how we can continue to make it inclusive for all.",
    "speakers": [
      "Jun Heider"
    ]
  },
  "live-video-streaming-as-tail-f": {
    "title": "Live Video Streaming as \"tail -f\"",
    "description": "Most live video delivery solutions today are built using DASH/HLS or similar delivery formats with “classic“ CDN infrastructure providing scalability and players with relatively complex logic that includes fetching manifest and segments from network, estimating bandwidth, making ABR decisions.<br><br> In this talk we would like to present an alternative architecture for live video streaming which draws inspiration from Linux ‘tail’ command. Imagine a client can “subscribe” to a live video stream and just render audio/video data as it comes from the network - sounds like a regular TV, right? And the framework can still support adaptive bitrates (ABR), support low latency and all features that DASH and HLS provide.<br><br>There are 4 major components in any live delivery system: ingest, processing/encoding, egress and players. In this talk, we going to cover last two. We will talk about network and application level protocol that we use to “pretend” that we tail a file, we discuss CDN components that we’ve built to be able to scale the system, we will also discuss how we can do ABR in this system and how we can control latency using HTTP-3 extensions that we’ve implemented.<br>",
    "speakers": [
      "Nitin Garg",
      "Kirill Pugin"
    ]
  },
  "the-quicematic-universe-season-20202021-preview": {
    "title": "The QUIC-ematic universe season 2020-2021 preview",
    "description": "QUIC is *the* secure UDP-based transport for the modern Internet. It&#39;s taken some time but in the 2019-2020 season, the IETF has edged ever closer to finishing &quot;version 1&quot;. Vendors have been progressively deploying the poster child -- HTTP/3 -- onto the Web and its going to soon be the new normal. <br><br>Now we need to see what&#39;s hot in the upcoming 2020-2021 season.  Eyes are turning to enhancements, extensions and new features.<br><br>At Demuxed 2019 we heard the often-rehashed orgin story of QUIC. This time we won&#39;t treat the audience like chumps. We&#39;ll focus on the new super powers of Datagram and WebTransport  a dynamic duo that break the rules and change the game. We&#39;ll also try a show not tell approach, which might be great, or might be crap. Imagine the Magician from that Dungeons and Dragons cartoon you vaguely remember.<br><br>By the end, you&#39;ll know some of their secrets and possibly be inspired to go and be your own hero, or anti-hero, in the marvelous QUIC-ematic universe.",
    "speakers": [
      "Lucas Pardue"
    ]
  },
  "delivering-better-video-encodes-for-legacy-devices": {
    "title": "Delivering better video encodes for legacy devices",
    "description": "At Netflix, we aim to deliver the best possible experience for all our members around the globe, no matter how or where they watch our content. For this reason, we support a wide variety of streaming devices, from lower end set-top-boxes to higher-end smart TVs, and maintain a variety of encode families. Over the years, we have introduced multiple innovations to adaptive streaming strategies and have constantly moved towards adopting more efficient codecs, such as AV1, the latest royalty-free standard. However, due to performance and decoder limitations of legacy devices, not all members are able to benefit from these quality and efficiency improvements. Instead, they often fall back to less efficient encodes, most notably our per-title H.264/AVC Main profile encode family. As a result, these encodes still represent a substantial portion of the streaming hours and network traffic<br><br>In this talk, we will provide an overview of changes to our encodes for legacy devices that have been recently rolled out. Compared to our per-title encodes, this approach lowers the bits streamed by over 20%, on average, while maintaining a similar level of perceived quality. This results in significant benefits across the entire delivery infrastructure and to the final experience for our members.",
    "speakers": [
      "Mariana Afonso"
    ]
  },
  "playback-on-edge--or-how-i-learned-to-stop-worrying-and-love-serverless": {
    "title": "Playback on Edge - Or how I learned to stop worrying and love serverless.",
    "description": "Will go through the trials and tribulation of building and deploying an edge solution for multi-cdn decisioning, manifest based server side ad insertion and playback orchestration on the &quot;edge&quot;. <br><br>We will cover: <br>* Going to the edge for delivery, not your sanity.<br>* What is playback orchestration?  <br><br>Cost / Performance metrics globally<br>* What does a edge performance journey look like.<br><br>Slicing HLS, Dicing Dash  <br>* Why Manifest level multi-cdn? <br>* Why Manifest level SSAI ?<br><br>Did someone say edge monolithic?<br>* Sub 10ms latency on edge lambdas means no micro-services hops - how to structure your project for single runtime.<br><br>Where to cache what and where? <br>* There are 2 hard things about engineering .. caching .. and I can&#39;t remember whats its called - How AWS Edge Lambda lifecycle works, what should be cached where.<br><br>* Lessons learned &amp; what’s next?  ",
    "speakers": [
      "Michael Dale"
    ]
  },
  "moving-live-video-quality-control-from-the-broadcast-facility-to-the-living-room": {
    "title": "Moving Live Video Quality Control from the Broadcast Facility to the Living Room",
    "description": "Live streaming has been an integral part of the video technology at Disney Streaming Services (DSS). DSS has multiple teams whose main functions are to preserve the highest possible video quality, and that the content transportation technologies are working as they should 24/7. In order to do this, DSS built and staffed Transmission Operations Centers (TOCs) in San Francisco and New York City, providing state-of-the-art video monitoring systems that allowed the TOC Staff to view and perform quality control on any of the thousands of IP-based streams that we might be processing at a given time.<br><br>As cases of COVID-19 in the United States increased, the decision was made to move the DSS workforce home. While this resulted in a variety of challenges for all of our employees, the challenges for the TOC Operators were unique.<br><br>The live video content that DSS uses for its sources tends to be high-bitrate, broadcast-quality streams. These streams are intended to be consumed via satellite or fiber. Some of them run in the neighborhood of 100 Mb/s. If people tried to consume these streams in their native format, the streams would be chewing up a huge amount of bandwidth, both on the DSS VPN and on users’ home internet connections. In order to allow TOC Operators to watch this content remotely, we had to generate proxy streams which would look good enough to monitor video, yet also reduce bandwidth to the point that any home internet connection could consume a few of these streams simultaneously. Aside from bandwidth, it was also important to provide these streams as close to real-time as possible, as our operators needed to see the streams “as they are,” not “as they were 30 seconds ago.”<br><br>Luckily, DSS has years of experience compressing high-quality video so that it can be consumed on devices at a low bitrate. DSS’s homegrown video transcoder, xCoder (the same transcoder which makes content for Disney+ and ESPN+, among other properties) had a solid foundation on which to build the Remote TOC monitoring platform. If you’ve ever watched anything on Disney+, ESPN+ or MLB.tv, you’d know that the video that xCoder produces looks amazing, so that provided a huge advantage. Using xCoder for this purpose, however, created a new set of challenges.<br><br>For both live and linear uses, xCoder was built to consume “groomed” transport streams. These streams are processed by DSS’s Broadcast Infrastructure environment, which standardizes incoming transport streams and sets the structures for Packet Identifiers (PIDs) and Program Map Tables (PMTs) to follow company standards. The advantage of having this environment is that it simplifies the setup of the xCoder for any given transcode job. Each 188-byte packet of data in a transport stream has a packet identifier (PID) that tells a receiving device what it is (video, audio, metadata, etc.). The PMT defines to a receiver of a transport stream what each of these PIDs are. Each elementary stream has its own PID number. With the thousands of streams out there, made by thousands of different encoders, the PID structures of each stream can be unique. Standardizing the structure in our broadcast environment allows the xCoders to just “worry” about decoding and encoding. TOC Operators needed to be able to monitor both groomed and ungroomed streams to make sure that this infrastructure is functioning correctly.<br><br>xCoder uses presets to determine the types of content coming in, and also the types of content to encode. One of the goals of the stream standardization system is to utilize as few presets as possible for a given partner. For example, for ESPN+, the system allows us to only have one preset for all events, from UFC to Ivy League Water Polo.<br><br>xCoder was built as part of a signal flow that takes in the groomed broadcast contribution sources on one end and produces video and audio segments for HTTP-based streaming on the other. In order to reduce latency, a few steps needed to be removed from that signal flow: the broadcast infrastructure that grooms the streams, the segmenter that generates the segments, the origin, and the CDN. As a bit of background, from an end user’s perspective, the segmenter actually produces and encrypts the media files that the viewer gets (short “chunks” of the video and audio). The CDN acts as a distributor of the media files so that everyone in the world trying to access the same media isn’t requesting from a single point, which would certainly cause a bottleneck. xCoder, as a standalone, produces a proprietary TCP stream. In order for the Remote TOC project to succeed, we needed a way for these streams to be transported to, and decoded by, users on their computers at home.<br><br>xCoder exposes an API hook that allows us to quickly determine things like frame rate, resolution, and even whether or not the xCoder is able to subscribe to the stream. This was originally made to be a troubleshooting tool, but we realized it could be repurposed to be the basis of the Remote TOC’s preset-on-the-fly routine.<br><br>After requesting stream information from the stream troubleshooting API, we could then present to the user all of the various elementary streams they might be interested in monitoring, then build a preset with that information and start up the proxy encode. This was the heart of this new workflow, and allowed operators to do their jobs without having to research any given stream in order to monitor it.<br><br>Before embarking on this project, DSS’s operators were staffing the TOC 24x7, ensuring that the live content for Hulu Live and ESPN+ looked great and were working as expected. This effort helped them to continue their 24x7 functions from the safety of their own homes, ensuring that the content for Hulu Live, ESPN+, and other properties could continue during this unprecedented global event.",
    "speakers": [
      "Michael Rappaport"
    ]
  },
  "low-latency-live-from-a-different-vantage-point": {
    "title": "Low Latency Live from a Different Vantage Point",
    "description": "Low latency live streaming using CMAF-based chunked encoding and transfer has, in one form or another, been one - sometimes &#39;the&#39; - subject du jour of the community in recent times. The competing proposals have various strengths and drawbacks that have been well dissected in previous talks and the emerging consensus thus far is that some variant of or co-existence of these approaches is where we will inevitably settle (in spite of the fact that these approaches introduce new complexity, require different adaptive bit rate control in players for good bandwidth adaptation, and in some cases, just can’t be supported without significant changes to origin servers or HTTP implementations). <br><br>In this talk, I will describe a different approach to the problem, one that starts from a different assumptions: That is, the root of the problem in realizing low-latency ABR streaming (to standard deployed players over HTTP) is not the streaming protocol per se but the constraints of traditional content distribution and transcoding pipelines. I will dissect the problems of the assumed “standard” architecture that works against the fundamental goals of achieving low latency live output to a large number of concurrent readers (streaming clients) in terms of a) the internal representation of media in the pipeline, b) file I/O constraints, c) application routing and micro service latencies, and d) piecemeal architecture. <br><br>I will contrast this with an alternative software substrate we’ve built that takes in standard live mezzanine streams (format agnostic) and yields ABR streaming to standard players (DASH, HLS) at scale using CMAF-based encoding, a decentralized real time publishing and routing layer, and just-in-time transcoding with minimal file I/O. I will include learnings regarding its performance for streaming to Internet audiences with high concurrency, in terms of stream bit rate, latency from the live source, infrastructure load and average player bit rate.",
    "speakers": [
      "Michelle Munson"
    ]
  },
  "dynamic-content-insertion-into-live-streams-across-the-3-main-streaming-formats--yes-that-includes-mss--single-period-dash": {
    "title": "Dynamic content insertion into live streams across the 3 main streaming formats - Yes, that includes MSS & single Period DASH!",
    "description": "As broadcasters and content providers widen the reach of supported devices and platforms, multiple streaming formats are required.  Inserting dynamic content, often targeted ads, is relatively straightforward for HLS and multi-Period DASH and there are a number of providers who can offer this. Enabling the broadest reach requires solving the problem for the large install base of legacy devices that only support Smooth Streaming or single Period DASH. <br><br>Targeted ad insertion services (DAI’s) or regional playout suites can be expensive for delivering regionalised live streams particularly when the regionalised content is promos and or opts.  It is possible to regionalise in the streaming domain without adding playout suites or using DAI.  You can even generate compliance recordings.<br><br>In this session, we will walk through how we successfully delivered dynamic content insertion for regionalisation purposes into HLS, DASH and Smooth Streaming live streams from a single global playout source and with a single encode.<br><br>We’ll share the highlights of our journey, including:<br><br>-The technical requirements for the source streams and the encoder output<br>-How signalling must be configured and inserted using SCTE-35 messages, and how it is included in the encoder output<br>-The custom changes made within the packaging layer for each of HLS, DASH and Smooth Streaming to dynamically insert alternate content based on SCTE-35<br>-The functional constraints that stakeholders need to buy in to to make this approach viable<br>-The preparation and conditioning requirements / workflow for the insertable VOD content <br>-Suppress / forward markers to allow downstream layering of DAI opportunities on top of the regionalised output<br>-How to ensure QoE and how monitoring is addressed<br>-The challenges we overcame en-route to production deployment<br>-The complexities to future features / changes that have derived from the solution<br>-How the next generation implementation moves the insertion from the packaging domain to the encoding domain, which challenges it solves, and which new challenges are introduced.<br>",
    "speakers": [
      "Nigel Harniman"
    ]
  },
  "editing-video-simply-by-editing-text-in-the-browser-a-meta-story": {
    "title": "Editing video simply by editing text, in the browser: a meta story",
    "description": "There is a technical barrier to editing video which often needs the know-how of complex Non-Linear Editor (NLE) applications. But that barrier has now been significantly reduced by taking advantage of granular metadata, advances in web technologies, and a cameo from A.I.<br><br>By leveraging in and out timecode per word generated by automatic speech recognition, you can now edit video in the browser as easily as you would copy and paste text in a word processor.<br><br>Production teams collaborate to mark key soundbites, order them as they like, and lay the spine of their story. All from the web. Then the editor exports the XML – which automatically recompiles the sequence in the NLE – and focuses on the fine cut.   <br><br>This talk will go through the journey we took to implement a responsive and novel video editing experience from within the browser, including the challenges faced to provide real-time preview playback throughout the editing process, and managing seamless cutting from one video source to another. We may or may not have had to hack around HTML5 video player. ;)",
    "speakers": [
      "Roderick Hodgson"
    ]
  },
  "video-vectorization": {
    "title": "Video vectorization",
    "description": "We present a new kind of video transcoding called &quot;video vectorization&quot;, which transcodes video to a vector-graphics format and leverages open and existing standards such as SVG and OpenGL for hardware accelerated rendering on end users&#39; devices.<br><br>Doing so can enable bit rate reductions of over an order of magnitude for animated and screencast video content without the need for a new codec or decoder on end users&#39; devices.<br><br>We explain at a high level how the technology works, and provide some real world demos of vectorized cartoon and animated videos,  and provide bitrate / quality / speed /performance comparisons with AV1, H265, H264 and VP9.<br><br>We go over some of the advantages of the approach, including scalability and ultra-low bitrates, as well as some of the disadvantages - namely it&#39;s restricted applicability to &quot;vector-friendly&quot; content. <br><br>We show how vector graphics can be used in parallel with existing codecs within a single video stream for &quot;hybrid content&quot; (e.g. news tickers and sports scoreboards),  as well as how it can practically integrated into modern streaming architectures, from packaging vector streams within an MP4 container, to streaming via HLS/DASH, to integration into popular javascript players.<br>",
    "speakers": [
      "Sam Bhattacharyya"
    ]
  },
  "video-personalization-for-strength-training": {
    "title": "Video Personalization for Strength Training",
    "description": "When engineers at Tonal thought about the challenge of providing personalized and dynamic guidance for strength training, we realized that we needed more than canned audio and video that would rarely be relevant to what the user was doing. We also believe that a real person giving guidance and support is the best way to deliver guidance and motivation. Video was the obvious choice, but a set composition can&#39;t react to a user in real-time and can&#39;t be personalized to match the user&#39;s journey. Personalization would be limited to selecting the best video we have available for the user.<br><br>We&#39;ve developed a unique content creation and delivery system that allows flexible and personalized video content. The video technology built into Tonal works more like a live video editor than a typical video player. This enables our cloud AI infrastructure to deliver a personalized audio/video experience to the user. During production, audio and video are broken up into clips and we attach a variety of metadata about the contents of the clips. We have the ability to programmatically change any audio or video we insert into a workout. When we build the AV presentation for the user we choose the most relevant clips to insert into the workout. We can vary when and how much the coach appears. We can create progressions of form cues over the workout or even over the span of weeks or months based on the user journey. We take into account what we know about the user like their goals and previous workouts to adjust the curriculum and the audio and video experience. We can insert clips specific to the user&#39;s progress or goals in any workout when that information is most relevant. The system also supports dynamically inserting audio feedback based on any number of metrics we are collecting about the user in real-time. We use this for simple applications like calling out the number of reps left in a set, and also more complicated metrics like form correction. As Tonal is able to detect more and more, the system allows improving existing content via additional audio that can be applied to all the content in the library.<br><br>The talk would cover the details of this process.",
    "speakers": [
      "Scott White"
    ]
  },
  "webrtc-does-what-learn-how-robotics-subsecond-broadcast-and-game-streaming-is-powered": {
    "title": "WebRTC does what? Learn how robotics, sub-second broadcast and game streaming is powered",
    "description": "Learn all the powerful concepts that separate WebRTC from other streaming technology. Coming from RTMP/HLS/$X WebRTC feels like alien technology, this talk tries to solve that.<br><br>I am the creator of Pion WebRTC and author of WebRTC for the Curious. I will share all the interesting things I learned along the way. I have had a blast learning all the unique<br>things you can do with WebRTC that no other protocol can.<br><br>* NAT Traversal<br>You don&#39;t need a server to send video between two peers in different networks! I will run through the basics of NATs (mapping/filtering)<br>and then how WebRTC uses ICE to find the best route. Also how WebRTC allows you to switch routes on the fly, switching from Wifi -&gt; Cellular shouldn&#39;t stop your video!<br><br>* Congestion Control<br>Instead of providing multiple streams WebRTC mutates the encoder on the fly. The viewer to send statistics back to the sender and adjust the<br>state of the encoder. This means you get perfect bitrate for the link you have, and don&#39;t waste resources encoding multiple times.<br><br>* Mandatory E2E Security<br>WebRTC uses DTLS and SRTP. Demonstrate how a call connects, and how after DTLS is established the keying material is extracted.<br><br>* Lossy/Out-Of-Order Data Delivery<br>WebRTC DataChannels allow you to prioritize how data is delivered. This lets you combat HOL blocking, and choose the transmissions settings that work best for you.<br><br>* Multicast Peer Discovery<br>mDNS is a new feature of WebRTC. It replaces IP address with uuids that are pub/sub-ed across the LAN. This was originally designed as a privacy feature.<br>It has unlocked the ability for two peers to connect without even knowing their IP Address ahead of time.<br><br>* Available Everywhere<br>WebRTC is a standardized technology, and is available in lots of different forms. You can build stuff today using Pion, aiortc, GStreamer or Google&#39;s implementation.<br>Having full implementations (and not just bindings) means first class experiences when building in your language of choice.<br><br>Then I want to talk about the things that are being built. This will loop back to how the things I mentioned before are important.<br><br>* Remote game streaming<br>Talk about how DataChannels are important. Talk about how congestion control could be fatal, and adjusting the bitrate on the fly is important.<br><br>* Teleoperation<br>Talk about how TelloGo can be used to control a remote vehicle. You also have companies in the space that are controlling everything over DataChannel.<br>WebRTC allows multiple video feeds and combined with Congestion Control makes it super powerful.<br><br>* SSH without a jump box<br>ssh-p2p allows you to access a server behind a NAT/Firewall directly. The server establishes a temporary hole.<br>NAT Traversal is also powerful for all the metadata that could flow with the video.<br><br>* Protocol Bridging<br>WebRTC uses the same underlying technology and SIP, RTSP and RIST. Making WebRTC a great choice for serving these existing technologies in the browser.",
    "speakers": [
      "Sean DuBois"
    ]
  },
  "its-time-to-rebuild-your-player-with-web-components": {
    "title": "It's time to rebuild your player with web components",
    "description": "In 2010 I started building Video.js because I was excited to use web technologies (not Flash) to make video player controls, but it was still hard to create a *native*-feeling experience. In 2020, web components are completing that story, allowing new video-related elements to become first-class citizens in the browser, shared between players, and compatible with any javascript framework by default.<br><br>Topics covered:<br>* Custom elements as portable, extendable video controls<br>* Extensions of the &lt;video&gt; tag<br>* Easily embedding any existing web player with a custom element (instead of big clunky embed codes)<br>* Other video related opportunities like &lt;playlist&gt;",
    "speakers": [
      "Steve Heffernan"
    ]
  },
  "and-now-presenting-what-no-one-asked-for-how-to-live-stream-a-laserdisc": {
    "title": "And now presenting what no one asked for: How to live stream a laserdisc",
    "description": "It began with a new job, spare time, and a penchant for impractical archaic media formats. After joining the Daily.co engineering team earlier this year, I had been eager to find a reason to try out our RTMP streaming feature. But what is worthy of a live stream from my apartment? That was the question I was pondering when I tripped over a milk crate of laserdiscs in my living room. Of course it is possible to live stream a laserdisc, but why would anyone do it? This idea tickled me and thus the personal project to see if I could achieve what no one asked for was born. I had the laserdisc player, the aforementioned milk crate, and more cables than you can shake a stick at. I have since come into possession of no fewer than four capture cards and converters, a few of which have made it possible to capture audio and video from a laserdisc copy of the 1995 feature film, &quot;The Net&quot; and stream said audio and video live via a toy app using Daily &amp; Mux.<br><br>Some issues remain. Will I figure out how to fix the picture overscanning? Did I really need a separate audio capture device or will I try again to add gain to the hdmi signal? Watch this talk to find out the answers to these questions and more. If nothing else, you&#39;ll learn fun party facts like laserdiscs store analog video and both analog &amp; digital audio. And while I cannot say this talk will be better than other laserdisc focused talks, I can say it benefits from being virtual, as the risibly complex laserdisc player to laptop setup can only be demonstrated from the comfort of home. It&#39;s the talk you didn&#39;t know you wanted to see.",
    "speakers": [
      "Vanessa Pyne"
    ]
  },
  "av1-and-arm": {
    "title": "AV1 and ARM",
    "description": "In 2018, the Alliance for Open Media released its next-generation<br>Video Codec AV1. One year after thanks to the work done in dav1d and its adoption AV1 can be decoded in software nearly everywhere and with requirements not dissimilar for decoding HEVC. In 2020 we have hardware decoders for the newer GPUS.<br><br>ARM is an architecture famous for its ubiquitous deployment in the<br>low-power, mobile devices and for some new entries in the server and HPC market. Now, with Apple spearheading it, the ARM will see even wider adoption in the laptop market and larger offers in the cloud market will come from Amazon, Packet.com and other players.<br><br>We will talk on how well AV1 runs on ARM.<br><br>We will talk a little about dav1d We will talk a lot about rav1e,<br>libaom and SVT-AV1 We will discuss what&#39;s missing to make AV1 shine on ARM.",
    "speakers": [
      "Vibhoothi"
    ]
  },
  "decoder-complexity-aware-av1-encoding-optimization": {
    "title": "Decoder Complexity Aware AV1 Encoding Optimization",
    "description": "AV1 has included more than 100+ coding tools compared to its predecessor VP9. AV1 hence left an impression to the community that yes, it is more coding efficient, but it is slow, fairly slow. Indeed, great thanks to our open source communities, AV1 encoders have been constantly optimized w.r.t. both its coding efficiency and encoding speed. To date, there are three well known open-source AV1 encoders: SVT-AV1 initiated by the joint effort between Intel and Netflix, and has also been specified by AOM SIWG as the version-0 codebase; rav1e initiated by Mozilla, as well as libaom initiated by AOMedia. For instance, from January 2019 through April 2020, just checking libaom by example, the AV1 encoder has been speedup by &gt;10x (not 10%), and the coding efficiency, quantified by the well acknowledged BD-rate figures, has also been improved by &gt;10% concurrently.<br><br>Also, sincere thanks to the AOMedia funded dav1d endeavor, an open AV1 software decoder initially jointly developed by VideoLan, Mozilla, and Two Orioles, the launching of AV1 as real products within 2 years of the finalization of the standard is not a far-fetched dream anymore, before the wide availability of hardware decoders in the market. The continuous optimization of dav1d opens the door to AV1, and feasible user scenarios are being identified where software decoding solutions can be deployed and accepted.<br><br>In this talk, we will address the AV1 encoding optimization from a different angle, namely decoder complexity aware AV1 encoding optimization. As we hone our strategies and algorithms to optimize AV1 encoders, in general, we are all trying on more superior coding efficiency well outperforming existing solutions, and meanwhile accelerating the encoding speed ready for a real product. Indeed, the awareness of the decoder complexity can in return collect valuable feedbacks to the AV1 encoder design, so that an AV1 encoder not only can achieve a sufficient good tradeoff between coding efficiency and encoding speed, but also help release the burden on the decoder side, to make the deployment of AV1 to an even wider community. <br><br>We will first have a brief overview on current performance for dav1d, including stats collected specifically regarding its decoding speed and power consumption. The decoding speed of dav1d has been consistently confirmed to outperform all other existing AV1 software decoders. To check its performance over mobile phones, we have evaluated dav1d against the ffmpeg-h264 software decoder and the openhevc software decoder, in terms of CPU usage [%],  memory  using [MB],  current [mA], consumed power[mW], voltage [mV], and temperature [℃], over several typical mobile devices. Our results demonstrate that power consumption wise, dav1d is ranked between the aforementioned two decoders, worse than ffmpeg-h264 but better than openhevc.<br><br>Furthermore, we worked with our collaborators, to collect extensively the decoding speed stats running dav1d particularly over low end mobile devices. For instance, our collaborator selected 18 common test sequences, used SVT-AV1 Preset 6 to generate AV1 bitstream, and decoded them on XiaoMi 4, with Qualcomm Snapdragon 801 Quad-core processor. The test set of videos include resolution of 720x536, 960x536, 960x480, 1024x576, and 2024x536. It can be observed that at higher encoding bitrate, the use of decoder on such types of devices would be very challenging to decode in real time. <br><br>Then we will address several coding tools that may directly affect the performance of the decoder complexity. We will then demonstrate the results that for specific content, certain coding tools may incur a large complexity at the decoder side whereas the encoding efficiency may only suffer a minimum degradation if such tools are skipped at the encoder side. For instance, AV1 includes block sizes ranging from 4x4 all the way to 128x128. By turning off extremely large partitions or the smallest partitions, a good trade-off may be achieved for a sufficiently good coding efficiency while incur not too much decoder side computational complexity, thus to have the decoder more capable of decoding in real time even over low end devices. The decoder complexity aware encoding design concept would further help drive the launching of AV1 forward. ",
    "speakers": [
      "Zoe Liu"
    ]
  }
}
